{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Emails and Financial Data\n",
    "### Background\n",
    "Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. And it was one of the largest companies in the United States in 2000. By 2002, it had collapsed into bankruptcy due to large amount of accounting and corporate corporate fraud. Its collapse affected thousands of its employees, shook Wall Street and influenced the whole Western economic system.\n",
    "\n",
    "In this project, a person of interest identifier will be built using the financial and email data made public as a result of the Enron scandal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Task 1: Select what features you'll use.\n",
    "# features_list is a list of strings, each of which is a feature name.\n",
    "# The first feature must be \"poi\".\n",
    "poi = ['poi']\n",
    "financial_features_list = ['salary', \n",
    "                           'deferral_payments', \n",
    "                           'total_payments', \n",
    "                           'loan_advances', 'bonus', \n",
    "                           'restricted_stock_deferred',\n",
    "                           'deferred_income', \n",
    "                           'total_stock_value', \n",
    "                           'expenses', \n",
    "                           'exercised_stock_options', \n",
    "                           'other', \n",
    "                           'long_term_incentive', \n",
    "                           'restricted_stock', \n",
    "                           'director_fees'] \n",
    "email_features_list = ['to_messages', \n",
    "                       'email_address',\n",
    "                       'from_poi_to_this_person',\n",
    "                       'from_messages', \n",
    "                       'from_this_person_to_poi',\n",
    "                       'shared_receipt_with_poi']\n",
    "    \n",
    "features_list = poi + financial_features_list +email_features_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of data points:  146\n",
      "Number of Person of Interest:  18\n",
      "Number of non Person of Interest:  128\n",
      "Precetage of Person of Interest in the dataset:  14.0625 %\n"
     ]
    }
   ],
   "source": [
    "# Total number of data point\n",
    "print\"\"\n",
    "print \"Number of data points: \", len(data_dict.keys())\n",
    "\n",
    "# Allocation across classes (POI/ non-POI)\n",
    "poi = 0\n",
    "for people in data_dict:\n",
    "    if data_dict[people]['poi'] == 1:\n",
    "        poi += 1\n",
    "non_poi = len(data_dict.keys()) - poi\n",
    "print \"Number of Person of Interest: \", poi\n",
    "print \"Number of non Person of Interest: \", non_poi\n",
    "print \"Precetage of Person of Interest in the dataset: \", float(poi)/ non_poi*100,\"%\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 146 number of data points, but only 18(14.06%) people are the Person of Interest(POI). The disproportionate number of class instances will arise the class imbalance problem, a common problem in machine learning that the total number of a class of data (positive) is far less than the total number of another class of data (negative). \n",
    "\n",
    "###  Alternative Metrics to Evaluate Algorithm Performance\n",
    "\n",
    "To address the problem, instead of using general accuracy of counting number of mistales, an aternative metrics (Precision, Recall and F1 score) will be used. Precision is used to determine the fraction of relevant instances (True Positive) among the retrieved instances (True Positive + False Positive). In this case, a great precision will be whenever a POI gets flagged in the test set, we have confidence that it is a real POI but not a false alarm. \n",
    "\n",
    "On the other hand, recall is used to detemine the fraction of relevant instances (True Positive) that have been retrieved over total relevant instances (True Positive + False Negative). For example, a great recall will be the identifier can successfully identify the POI everytimes he/ she shows up in the test set. \n",
    "\n",
    "F1 score is the a measure that combines precision and recall and is the harmonic mean of precision and recall. A great F1 score what we prefer as both the False Positive abd False Negative rates are low. In such case, if the identifier finds a POI, then the person is almost certainly a POI; and if the identifier does not flag someone, then he/ she are almost certainly not a POI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  21\n",
      "Missing values of each features:\n",
      "                           Missing value  Percentage\n",
      "loan_advances                        142    0.972603\n",
      "director_fees                        129    0.883562\n",
      "restricted_stock_deferred            128    0.876712\n",
      "deferral_payments                    107    0.732877\n",
      "deferred_income                       97    0.664384\n",
      "long_term_incentive                   80    0.547945\n",
      "bonus                                 64    0.438356\n",
      "to_messages                           60    0.410959\n",
      "shared_receipt_with_poi               60    0.410959\n",
      "from_messages                         60    0.410959\n",
      "from_poi_to_this_person               60    0.410959\n",
      "from_this_person_to_poi               60    0.410959\n",
      "other                                 53    0.363014\n",
      "salary                                51    0.349315\n",
      "expenses                              51    0.349315\n",
      "exercised_stock_options               44    0.301370\n",
      "restricted_stock                      36    0.246575\n",
      "email_address                         35    0.239726\n",
      "total_payments                        21    0.143836\n",
      "total_stock_value                     20    0.136986\n",
      "poi                                    0    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Number of features used\n",
    "print \"Number of features: \", (len(data_dict[people].keys()))\n",
    "\n",
    "# Missing values\n",
    "missing_dict= {}\n",
    "for features in data_dict[people]:\n",
    "    missing_dict[features] = 0\n",
    "\n",
    "for people in data_dict:\n",
    "    for features in data_dict[people]:\n",
    "        if data_dict[people][features] == \"NaN\":\n",
    "            missing_dict[features] += 1\n",
    "\n",
    "print \"Missing values of each features:\"\n",
    "\n",
    "name_list = []\n",
    "score_list = []\n",
    "percentage_list = []\n",
    "for name, score in sorted(missing_dict.iteritems(), key=lambda item:item[1], reverse=True):\n",
    "    name_list.append(name)\n",
    "    score_list.append(score)\n",
    "for value in score_list:\n",
    "    percentage_list.append(value/146.0)\n",
    "import pandas as pd\n",
    "missing_value_table = pd.DataFrame({\"Missing value\": score_list, \"Percentage\": percentage_list}, index = name_list)\n",
    "print missing_value_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 21 features. These features inlcude financial features, email_features and the label of whether the person is a person of interest. Almost of these features have missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFrxJREFUeJzt3XuQXOV55/Hvo9ENBJIQkowQCAlW2AgQlx0DJjYByw4S\nxKWkFifCLl9YZ7XY4MVOrdesKxcSp7KVyiYOxGBZYELwZq3FCRujIJCrhLHjYLMMtxEyN0VgEBKR\nLNAIJJA0o2f/6NZxaxjNtC5nerrn+6mamj7vebv7eX3w/PSec/rtyEwkSQIY0egCJElDh6EgSSoY\nCpKkgqEgSSoYCpKkgqEgSSo0ZShExO0RsSkinqqj74yI+EFEPB4RnRFx2WDUKEnNqClDAbgDmF9n\n398D7srMc4BFwC1lFSVJza4pQyEzfwS8VtsWEadExP0R8WhE/HNEvGdvd2B89fEEYMMglipJTWVk\nows4jJYCV2fm8xFxPpUZwQeBG4DvR8TngXHAhxpXoiQNbS0RChFxFHAh8N2I2Ns8pvr7SuCOzPyL\niHgf8O2IOCMz9zSgVEka0loiFKicBtuamWf3se8zVK8/ZOZPImIsMBnYNIj1SVJTaMprCr1l5jbg\nhYj4KEBUnFXd/RIwr9p+GjAW2NyQQiVpiItmXCU1Ir4DXEzlX/z/Bvwh8ADwDWAaMApYlpl/HBFz\ngFuBo6hcdP5vmfn9RtQtSUNdU4aCJKkcLXH6SJJ0eDTdhebJkyfnzJkzG12GJDWVRx999BeZOWWg\nfk0XCjNnzqSjo6PRZUhSU4mIn9fTr7TTRwOtT1S9Q+imiFhbXZPo3LJqkSTVp8xrCnfQ//pEC4DZ\n1Z/FVO4ckiQ1UGmh0Nf6RL0sBO7Mip8CEyNiWln1SJIG1si7j6YDL9dsr6+2vUNELI6Ijojo2LzZ\nz51JUlma4pbUzFyame2Z2T5lyoAXzyVJB6mRdx+9ApxYs31CtU2SVKOzs5NVq1bR1dXFhAkTmDdv\nHnPnzi3lvRo5U7gH+GT1LqQLgK7M3NjAeiRpyOns7GT58uV0dXUB0NXVxfLly+ns7Czl/UqbKdSu\nTxQR66msTzQKIDOXACuAy4C1wA7gqrJqkaRmtWrVKnbv3r1P2+7du1m1alUps4XSQiEzrxxgfwLX\nlPX+ktQK9s4Q6m0/VE1xoVmShqsJEyYcUPuhMhQkaQibN28eo0aN2qdt1KhRzJs3r5T3a7q1jyRp\nONl73WCw7j4yFCRpiJs7d25pIdCbp48kSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJU\nMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQk\nSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSYVSQyEi5kfEsxGxNiKu72P/hIhYHhFPRsSaiLiq\nzHokSf0rLRQiog24GVgAzAGujIg5vbpdA/wsM88CLgb+IiJGl1WTJKl/Zc4UzgPWZua6zNwFLAMW\n9uqTwNEREcBRwGtAd4k1SZL6UWYoTAdertleX22r9XXgNGADsBq4LjP39H6hiFgcER0R0bF58+ay\n6pWkYa/RF5ovBZ4AjgfOBr4eEeN7d8rMpZnZnpntU6ZMGewaJWnYKDMUXgFOrNk+odpW6yrg7qxY\nC7wAvKfEmiRJ/SgzFB4BZkfErOrF40XAPb36vATMA4iIdwHvBtaVWJMkqR8jy3rhzOyOiGuBlUAb\ncHtmromIq6v7lwBfBe6IiNVAAF/OzF+UVZMkqX+lhQJAZq4AVvRqW1LzeAPwa2XWIEmqX6MvNEuS\nhhBDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVD\nQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJUMBQkSQVDQZJU\nMBQkSQVDQZJUMBQkSQVDQZJUMBQkSYVSQyEi5kfEsxGxNiKu30+fiyPiiYhYExE/LLMeSVL/Rpb1\nwhHRBtwMfBhYDzwSEfdk5s9q+kwEbgHmZ+ZLETG1rHokSQMrc6ZwHrA2M9dl5i5gGbCwV5+PAXdn\n5ksAmbmpxHokSQMoMxSmAy/XbK+vttU6FTgmIh6MiEcj4pN9vVBELI6Ijojo2Lx5c0nlSpIafaF5\nJPDvgcuBS4Hfj4hTe3fKzKWZ2Z6Z7VOmTBnsGiVp2CjtmgLwCnBizfYJ1bZa64Etmbkd2B4RPwLO\nAp4rsS5J0n6UOVN4BJgdEbMiYjSwCLinV5/vAe+PiJERcSRwPvB0iTVJkvpR2kwhM7sj4lpgJdAG\n3J6ZayLi6ur+JZn5dETcD3QCe4DbMvOpsmqSJPUvMrPRNRyQ9vb27OjoaHQZktRUIuLRzGwfqF+j\nLzRLkoYQQ0GSVDAUJEkFQ0GSVDAUJEkFQ0GSVKgrFCLioxFxdPXx70XE3RFxbrmlSZIGW70zhd/P\nzDci4v3Ah4BvAd8oryxJUiPUGwo91d+XA0sz815gdDklSZIapd5QeCUivgn8NrAiIsYcwHMlSU2i\n3j/sv0VlDaNLM3MrMAn4UmlVSZIaot4F8SYDHQARMaPa9kwpFUmSGqbeULgXSCCAscAs4Fng9JLq\nkiQ1QF2hkJln1m5Xb0f9XCkVSZIa5qAuFmfmY1S+EEeS1ELqmilExO/WbI4AzgU2lFKRJKlh6r2m\ncHTN424q1xj+4fCXI0lqpHqvKfxR2YVIkhqv3tNHpwL/FZhZ+5zM/GA5ZUmSGqHe00ffBZYAt/HL\nJS8kSS2m3lDozkwXwJOkFlfvLanLI+JzETEtIibt/Sm1MknSoKt3pvCp6u/a9Y4SOPnwliNJaqR6\n7z6aVXYhkqTGq/fuo1HAZ4GLqk0PAt/MzN0l1SVJaoB6Tx99AxgF3FLd/kS17XfKKEqS1Bj1hsJ7\nM/Osmu0HIuLJMgqSJDVO3V/HGRGn7N2IiJPx8wqS1HLqnSl8CfhBRKyrbs8EriqlIklSw9Q7U/gX\n4JvAHuC16uOflFWUJKkx6g2FO6l829pXgb+m8vmEb5dVlCSpMeoNhTMy83cy8wfVn/9EHV/FGRHz\nI+LZiFgbEdf30++9EdEdEVfUW7gk6fCrNxQei4gL9m5ExPlAR39PiIg24GZgATAHuDIi5uyn358B\n36+3aElSOfq90BwRq6ksZzEKeCgiXqpunwQ8M8Brnweszcx11ddaBiwEftar3+epfGHPew+4eknS\nYTXQ3Ue/fgivPR14uWZ7Pb2+1zkipgO/CVxCP6EQEYuBxQAzZsw4hJIkSf3pNxQy8+clv/9fAV/O\nzD0R0V8dS4GlAO3t7VlyTZI0bNX7OYWD8QpwYs32CdW2Wu3AsmogTAYui4juzPzHEuuSJO1HmaHw\nCDA7ImZRCYNFwMdqO9SuvhoRdwD/ZCBIUuOUFgqZ2R0R1wIrgTbg9sxcExFXV/cvKeu9JUkHp8yZ\nApm5AljRq63PMMjMT5dZiyRpYPV+TkGSNAwYCpKkgqEgSSoYCpKkgqEgSSoYCpKkgqEgSSoYCpKk\ngqEgSSoYCpKkgqEgSSoYCpKkgqEgSSoYCpKkgqEgSSoYCpKkgqEgSSoYCpKkgqEgSSoYCpKkgqEg\nSSoYCpKkgqEgSSoYCpKkgqEgSSoYCpKkgqEgSSoYCpKkgqEgSSoYCpKkgqEgSSqUGgoRMT8ino2I\ntRFxfR/7Px4RnRGxOiIeioizyqxHktS/0kIhItqAm4EFwBzgyoiY06vbC8CvZuaZwFeBpWXVI0ka\nWJkzhfOAtZm5LjN3AcuAhbUdMvOhzHy9uvlT4IQS65EkDaDMUJgOvFyzvb7atj+fAe7ra0dELI6I\njojo2Lx582EsUZJUa0hcaI6IS6iEwpf72p+ZSzOzPTPbp0yZMrjFSdIwMrLE134FOLFm+4Rq2z4i\nYi5wG7AgM7eUWI8kaQBlzhQeAWZHxKyIGA0sAu6p7RARM4C7gU9k5nMl1iJJqkNpM4XM7I6Ia4GV\nQBtwe2auiYirq/uXAH8AHAvcEhEA3ZnZXlZNkqT+RWY2uoYD0t7enh0dHY0uQ5KaSkQ8Ws8/uofE\nhWZJ0tBgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaCJKlgKEiSCoaC\nJKlgKEiSCoaCJKlQ5tdxDnnbH9/EtpUv0rN1J20TxzD+0pmMO2dqo8uSpIYZtqGw/fFNbL37eXL3\nHgB6tu5k693PAxgMkoatYXv6aNvKF4tA2Ct372Hbyhf37dh5F3ztDLhhYuV3512DV6QkDbJhO1Po\n2bpz4PbOu2D5f4Hdb1W2u16ubAPM/a2SK5SkwTdsZwptE8cM3L7qj38ZCHvtfqvSLkktaNjOFEaM\ne56uu24hd7xGHDGJ0XN+kzEnv4/xl878Zaeu9XS9eASbOo+me0cbI4/sYercN5gwc33D6pakMg3L\nmULX8uW8/q3/Se54DYB86zV2PvFtRk59cZ+LzF2bjmfjIxPo3jESCLp3jGTjIxPo2nR8gyqXpHIN\ny1DY9LW/It9+e9/Gnl1s+7+379uvczzZs+//RNkzgk2d48suUZIaYliePureuJEHpp/D356+gM1H\nHMOUt17nU2vu44Mbnti335ZtfT9/P+2S1OyGZSj88PSLuWnmh9k5cjQAm46cxE3nfJS2YyZyWk2/\nkdOm0b1hwzueP3LatEGqVJIG17A8ffS3p19WBMJeO0eO5rbZF/L00r9k++ObAJj6xS8QY8fu0y/G\njmXqF78waLVK0mAaljOFV3cGALPfeI4LX3+Yo3ve5I22o3jomPPZOOtW8sc9nMSnmPCRjwCVaxDd\nGzcycto0pn7xC0W7JLWaYRkKx088giNffpJ5W37IqOwGYHzPm3xoy4NsWTeWtpO/yzErf5Vx50zl\nx6eP4MbPtfHq9pEcN66N604fweUNrl+SyjIsTx996dJ38yuvP1wEwl4js4eN/28qbW9PpGfrTu5d\ndy83PHQDG7dvJEk2bt/IDQ/dwL3r7m1Q5ZJUrmE5U5j0vVs5qudN2HEkF61/nqPeeouRR/ZwxBm7\neHLEVCa9uY04oo0bH7uRt3v2vXX17Z63ufGxG7n8ZOcLklrPsAyFB3r2cOyo0bznjbE8edZX2Dlm\nEmN2vsbJL36PC056gIn/uoMHNv13Xp35Wp/Pf3X7q4NcsSQNjlJPH0XE/Ih4NiLWRsT1feyPiLip\nur8zIs4tsx6A377pdxnzrm3M+LdjWDv7SnaOPRYi2Dn2WJ6d/XEe33AJbSOS8yY9zbi3+s7M48Yd\nV3aZktQQpYVCRLQBNwMLgDnAlRExp1e3BcDs6s9i4Btl1QPw6Zu/wutjk7knP8Dm6fPZ07bvonh7\n2sbw4vTfAGD8qJ2c88wERvb6RPPYtrFcd+51ZZYpSQ1T5kzhPGBtZq7LzF3AMmBhrz4LgTuz4qfA\nxIgo7ZNhT4/6EWdumcqYMdvZOWZSn332tm/bPYZTNh7F+1ZPYtq4aQTBtHHTuOHCG7yeIKlllXlN\nYTrwcs32euD8OvpMBzbWdoqIxVRmEsyYMeOgC9oxuoujdu5k585x7BqxldF5zDv6dI94jZ49wY83\nzwTg7F0zueWKvzno95SkZtIUt6Rm5tLMbM/M9ilTphz06xy5awJvjhnDiy+czYNnbWEPu/bZv4ed\nvHDK09y/8VSe2TaVkaPH8IFFnzzU8iWpaZQZCq8AJ9Zsn1BtO9A+h81puy9i9bGb2LDl3zFt3Gbu\nO/tVto3ZSpJsG7OV++ZuYsfbbTzzxrs4evIUfm3xtZz2gUvKKkeShpwyTx89AsyOiFlU/tAvAj7W\nq889wLURsYzKqaWuzNxISe645k/59M1f4bGpb3D6+klMHL+b2xZMY/vosYzf1cb/OPti/sNxfV9r\nkKThoLRQyMzuiLgWWAm0Abdn5pqIuLq6fwmwArgMWAvsAK4qq5697rjmT8t+C0lqWqV+eC0zV1D5\nw1/btqTmcQLXlFmDJKl+TXGhWZI0OAwFSVLBUJAkFQwFSVLBUJAkFQwFSVLBUJAkFaLyUYHmERGb\ngZ8fhpeaDPziMLzOUNXq44PWH2Orjw9af4xDaXwnZeaAi8c1XSgcLhHRkZntja6jLK0+Pmj9Mbb6\n+KD1x9iM4/P0kSSpYChIkgrDORSWNrqAkrX6+KD1x9jq44PWH2PTjW/YXlOQJL3TcJ4pSJJ6MRQk\nSYWWDoWImB8Rz0bE2oi4vo/9ERE3Vfd3RsS5jajzUNQxxosjoisinqj+/EEj6jxYEXF7RGyKiKf2\ns7+pj2Ed42vq4wcQESdGxA8i4mcRsSYiruujT9MexzrH1zzHMTNb8ofKt739K3AyMBp4EpjTq89l\nwH1AABcADze67hLGeDHwT42u9RDGeBFwLvDUfvY3+zEcaHxNffyqY5gGnFt9fDTwXCv9f7HO8TXN\ncWzlmcJ5wNrMXJeZu4BlwMJefRYCd2bFT4GJETFtsAs9BPWMsall5o+A1/rp0tTHsI7xNb3M3JiZ\nj1UfvwE8DUzv1a1pj2Od42sarRwK04GXa7bX884DVU+foaze+i+sTsnvi4jTB6e0QdPsx7AeLXP8\nImImcA7wcK9dLXEc+xkfNMlxLPU7mjUkPAbMyMw3I+Iy4B+B2Q2uSfVrmeMXEUcB/wB8ITO3Nbqe\nw22A8TXNcWzlmcIrwIk12ydU2w60z1A2YP2ZuS0z36w+XgGMiojJg1di6Zr9GParVY5fRIyi8gfz\n7zLz7j66NPVxHGh8zXQcWzkUHgFmR8SsiBgNLALu6dXnHuCT1TsfLgC6MnPjYBd6CAYcY0QcFxFR\nfXwelWO+ZdArLU+zH8N+tcLxq9b/LeDpzPzL/XRr2uNYz/ia6Ti27OmjzOyOiGuBlVTu0rk9M9dE\nxNXV/UuAFVTuelgL7ACualS9B6POMV4BfDYiuoG3gEVZvR2iGUTEd6jcuTE5ItYDfwiMgtY4hnWM\nr6mPX9WvAJ8AVkfEE9W2rwAzoCWOYz3ja5rj6DIXkqRCK58+kiQdIENBklQwFCRJBUNBklQwFCRp\nCBto0cRefb9Ws+jecxGx9YDfz7uPpIMXEXdQWejs7xtdi1pTRFwEvEllbagzDuB5nwfOycz/eCDv\n50xBGkQR0bKfDVI5+lo0MSJOiYj7I+LRiPjniHhPH0+9EvjOgb6f/4FKvUTEOOAuKksttAFfBd4N\nfAQ4AngI+M+9P3xUXSP/HX0i4kHgCeD9wPKI+DRwambujojxVJY8PzUzdw/C8NQalgJXZ+bzEXE+\ncAvwwb07I+IkYBbwwIG+sDMF6Z3mAxsy86zqdP1+4OuZ+d7q9hHAr/fxvP76jM7M9sz8I+BB4PJq\n+yLgbgNB9aouvHch8N3qJ6i/SeU7HWotAv4+M3sO9PUNBemdVgMfjog/i4gPZGYXcElEPBwRq6n8\ni6yvpY/76/N/ah7fxi+XcbgK+JvDPwS1sBHA1sw8u+bntF59FnEQp472vrikGpn5HJVvQ1sN/En1\ntNAtwBWZeSZwKzC29jkRMXaAPttrXv9fgJkRcTHQlpkD3lUi7VVdlvuFiPgoFF9letbe/dXrC8cA\nPzmY1zcUpF4i4nhgR2b+L+DPqQQEwC+qU/cr+nja2Dr61LoT+N84S9AAqosm/gR4d0Ssj4jPAB8H\nPhMRTwJr2PcbFxcByw52wT0vNEvvdCbw5xGxB9gNfBb4DeAp4FUqS5bvIzO3RsSt/fXp5e+AP+Eg\np/gaPjLzyv3smr+f/jccyvv5OQWpASLiCmBhZn6i0bVItZwpSIMsIv4aWEDl+wOkIcWZgiSp4IVm\nSVLBUJAkFQwFSVLBUJAkFQwFSVLh/wP1B+q3c2iFtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x457f710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "\n",
    "### your code below\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    matplotlib.pyplot.scatter( salary, bonus )\n",
    "\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data point:  143\n"
     ]
    }
   ],
   "source": [
    "### Task 2: Remove outliers \n",
    "data_dict.pop(\"TOTAL\", \"None\" )\n",
    "\n",
    "# Cleaning the data\n",
    "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", \"None\" )\n",
    "data_dict.pop(\"LOCKHART EUGENE E\", \"None\" )\n",
    "print \"Data point: \", len(data_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEKCAYAAABQRFHsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEYmVEpGIiMRKiUhERGKlRCQiIrH6/4lw3FIdlKZsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb830198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "\n",
    "### your code below\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    matplotlib.pyplot.scatter( salary, bonus )\n",
    "\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot has clearly shows that there is a hugh outlier generated by the \"Total\" row, so I removed it from the dataset. I also removed the information of the \"THE TRAVEL AGENCY IN THE PARK\" as it is an agency but not an employee.\n",
    "Futhermore, I removed the entry of \"LOCKHART EUGENE E\" as it contains only missing values. After clearning the data, the dataset remains 143 data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def performance(clf_u, dataset, the_features_list):\n",
    "    # Extract features and labels from dataset for local testing\n",
    "    data = featureFormat(dataset, the_features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    \n",
    "    features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "       \n",
    "    ### fit the classifier using training set, and test on test set\n",
    "    clf_u.fit(features_train, labels_train)\n",
    "    predictions = clf_u.predict(features_test)\n",
    "    for prediction, truth in zip(predictions, labels_test):\n",
    "        if prediction == 0 and truth == 0:\n",
    "            true_negatives += 1\n",
    "        elif prediction == 0 and truth == 1:\n",
    "            false_negatives += 1\n",
    "        elif prediction == 1 and truth == 0:\n",
    "            false_positives += 1\n",
    "        elif prediction == 1 and truth == 1:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "            print \"All predictions should take value 0 or 1.\"\n",
    "            print \"Evaluating performance for processed predictions:\"\n",
    "            break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        return clf_u, accuracy, precision, recall, f1, f2\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf_u\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "        \n",
    "# print the performance\n",
    "def print_performance(clf, accuracy, precision, recall, f1, f2):\n",
    "    print \"Performance of\", clf\n",
    "    print \"     Accuracy:\", accuracy, \",Precision:\", precision, \",Recall:\", recall, \",F1:\", f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_list.remove(\"email_address\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preformance before and after adding the new features:\n",
      "                            Accuracy        F1  Precision  Recall\n",
      "Before adding new features  0.883721  0.444444        0.5     0.4\n",
      "After adding new features   0.883721  0.444444        0.5     0.4\n",
      "Improved:                  -0.000000 -0.000000       -0.0    -0.0\n"
     ]
    }
   ],
   "source": [
    "# performance before adding new features\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_bnf = GaussianNB()\n",
    "clf_bnf, accuracy_b, precision_b, recall_b, f1_b, f2_b = performance(clf_bnf, data_dict, features_list)\n",
    "\n",
    "# Task 3: Create new feature(s)\n",
    "def computeFraction(poi_messages, all_messages):\n",
    "    \"\"\" given a number messages to/from POI (numerator) \n",
    "        and number of all messages to/from a person (denominator),\n",
    "        return the fraction of messages to/from that person\n",
    "        that are from/to a POI\n",
    "   \"\"\"\n",
    "    if poi_messages != \"NaN\" and all_messages != \"NaN\":\n",
    "        fraction = float(poi_messages)/ all_messages\n",
    "        \n",
    "    else:\n",
    "        fraction = 0\n",
    "    return fraction\n",
    "\n",
    "for name in data_dict:\n",
    "    data_point = data_dict[name]\n",
    "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
    "    to_messages = data_point[\"to_messages\"]\n",
    "    fraction_from_poi = computeFraction(from_poi_to_this_person, to_messages)\n",
    "    data_point[\"fraction_from_poi\"] = fraction_from_poi\n",
    "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
    "    from_messages = data_point[\"from_messages\"]\n",
    "    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages)\n",
    "    data_point[\"fraction_to_poi\"] = fraction_to_poi\n",
    "features_list = features_list + [\"fraction_from_poi\"] + [\"fraction_to_poi\"] \n",
    "\n",
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# performance after adding new features\n",
    "clf_anf = GaussianNB()\n",
    "clf_anf, accuracy, precision, recall, f1, f2 = performance(clf_anf, my_dataset, features_list)\n",
    "\n",
    "# compareing the performance before and after adding the new features \n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list =[]\n",
    "f1_list =[]\n",
    "features_status_list = [\"Before adding new features\", \"After adding new features\", \"Improved:\"]\n",
    "accuracy_list = [accuracy_b, accuracy, -(accuracy_b - accuracy)]\n",
    "precision_list = [precision_b, precision, -(precision_b - precision)]\n",
    "recall_list =[recall_b, recall, -(recall_b - recall)]\n",
    "f1_list =[f1_b, f1, -(f1_b - f1)]\n",
    "     \n",
    "status_table_1 = pd.DataFrame({\"Accuracy\": accuracy_list, \"Precision\": precision_list, \n",
    "                             \"Recall\": recall_list, \"F1\": f1_list}, index = features_status_list)\n",
    "print \"The preformance before and after adding the new features:\"\n",
    "print status_table_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since people who have higher frequencies of receiving and sending email with Persons of Interest may also be a Person of Interest too, I create two new features(\"fraction_from_poi\", \"fraction_to_poi\") which caputure the fraction of emails the person sent to Persons of Interest and the fraction of emails received from Persons of interest. Although the performance has not improved in the mean time, one of the new features, \"fraction_to_poi\", turns out to be relatively important and has been choosed in the SelectKBest feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features= scaler.fit_transform(features)\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preformance before and after feature selection:\n",
      "                        Accuracy        F1  Precision  Recall\n",
      "Before feature scaling  0.883721  0.444444        0.5     0.4\n",
      "After feature scaling   0.883721  0.444444        0.5     0.4\n",
      "Improved:              -0.000000 -0.000000       -0.0    -0.0\n"
     ]
    }
   ],
   "source": [
    "clf_bfs = GaussianNB()\n",
    "clf_bfs, accuracy_s, precision_s, recall_s, f1_s, f2_s = performance(clf_bfs, my_dataset, features_list)\n",
    "\n",
    "features_status_list = [\"Before feature scaling\", \"After feature scaling\", \"Improved:\"]\n",
    "accuracy_list = [accuracy, accuracy_s, -(accuracy - accuracy_s)]\n",
    "precision_list = [precision, precision_s, -(precision - precision_s)]\n",
    "recall_list =[recall, recall_s, -(recall - recall_s)]\n",
    "f1_list =[f1, f1_s, -(f1 - f1_s)]\n",
    "     \n",
    "status_table_2 = pd.DataFrame({\"Accuracy\": accuracy_list, \"Precision\": precision_list, \n",
    "                             \"Recall\": recall_list, \"F1\": f1_list}, index = features_status_list)\n",
    "print \"The preformance before and after feature selection:\"\n",
    "print status_table_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Feature scaling is a method used to standardize the range of independent variables or features of data. The table shows the preformance does not improve after scaling the features as Gaussian Naive Bayes has been used as the classifier, and its parameter estimation doesn't involve optimization. The reason feature scaling has been performed as KMeans Clustering will be attempted to compare the performance between different algorithms, and this classfiwe will trade off one-dimension to the other when calculating the distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features and feature scores:\n",
      "                               Score\n",
      "exercised_stock_options    24.815080\n",
      "total_stock_value          24.182899\n",
      "bonus                      20.792252\n",
      "salary                     18.289684\n",
      "fraction_to_poi            16.409713\n",
      "deferred_income            11.458477\n",
      "long_term_incentive         9.922186\n",
      "restricted_stock            9.212811\n",
      "total_payments              8.772778\n",
      "shared_receipt_with_poi     8.589421\n",
      "loan_advances               7.184056\n",
      "expenses                    6.094173\n",
      "from_poi_to_this_person     5.243450\n",
      "other                       4.187478\n",
      "fraction_from_poi           3.128092\n",
      "from_this_person_to_poi     2.382612\n",
      "director_fees               2.126328\n",
      "to_messages                 1.646341\n",
      "deferral_payments           0.224611\n",
      "from_messages               0.169701\n",
      "restricted_stock_deferred   0.065500\n"
     ]
    }
   ],
   "source": [
    "# Univariate feature selection: SelectKBest\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "def select_k_best(data_dict, features_list, num_features):\n",
    "    data = featureFormat(data_dict, features_list)\n",
    "    label, features = targetFeatureSplit(data)\n",
    "\n",
    "    clf = SelectKBest(k = num_features)\n",
    "    clf = clf.fit(features, label)\n",
    "    features_weights = {}\n",
    "    for index, feature in enumerate(clf.scores_):\n",
    "        features_weights[features_list[1:][index]] = feature\n",
    "    best_features = sorted(features_weights.items(), key = lambda k: k[1], reverse = True)[:num_features]\n",
    "    new_features_list = []\n",
    "    score_list = []\n",
    "    for feature, score in best_features:\n",
    "        new_features_list.append(feature)\n",
    "        score_list.append(score)\n",
    "        #print feature, \": \", score\n",
    "    return new_features_list, score_list\n",
    "\n",
    "# Full list of features\n",
    "import pandas as pd\n",
    "new_features_list, score_list = select_k_best(data_dict, features_list, 21)\n",
    "select_k_best_table = pd.DataFrame({\"Score\": score_list}, index = new_features_list)\n",
    "print \"The features and feature scores:\"\n",
    "print select_k_best_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The univariate feature selection is deployed and the SelectKBest is used as the automated feature selection function. The result shows that the feature scores ranged from 24.82 to 0.07. The \"Exercised Stock Options\" feature has the highest score(24.82), while the \"Restricted Stock Deferred\" feature has the lowest score(0.07). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of different numbers of best features included in the model:\n",
      "    Accuracy        F1  Precision  Recall\n",
      "5   0.825000  0.222222       0.25     0.2\n",
      "6   0.880952  0.545455       0.60     0.5\n",
      "7   0.880952  0.545455       0.60     0.5\n",
      "8   0.880952  0.545455       0.60     0.5\n",
      "9   0.860465  0.400000       0.40     0.4\n",
      "10  0.883721  0.545455       0.50     0.6\n",
      "11  0.860465  0.400000       0.40     0.4\n",
      "12  0.860465  0.400000       0.40     0.4\n",
      "13  0.860465  0.400000       0.40     0.4\n",
      "14  0.860465  0.400000       0.40     0.4\n",
      "15  0.860465  0.400000       0.40     0.4\n",
      "16  0.860465  0.400000       0.40     0.4\n",
      "17  0.883721  0.444444       0.50     0.4\n",
      "18  0.906977  0.600000       0.60     0.6\n",
      "19  0.906977  0.600000       0.60     0.6\n",
      "20  0.883721  0.444444       0.50     0.4\n"
     ]
    }
   ],
   "source": [
    "# Select the best number of feaatures to include for training\n",
    "num_features_list = []\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list =[]\n",
    "f1_list =[]\n",
    "\n",
    "\n",
    "\n",
    "for number in range(5,21):\n",
    "    num_features_list.append(number)\n",
    "    testing_list = []\n",
    "    new_list, score =  select_k_best(my_dataset, features_list, number)\n",
    "    testing_list = [\"poi\"] + new_list\n",
    "    clf_tse = GaussianNB()\n",
    "    clf, accuracy, precision, recall, f1, f2 = performance(clf_tse, my_dataset, testing_list)\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "\n",
    "select_num_k_table = pd.DataFrame({\"Accuracy\": accuracy_list, \"Precision\": precision_list, \n",
    "                                   \"Recall\": recall_list, \"F1\": f1_list}, index = num_features_list )\n",
    "print \"The performance of different numbers of best features included in the model:\"\n",
    "print select_num_k_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After experimenting with including different numbers of best features in the model, the results shows that inlcuding 6/ 7/ 8 best features in the model provide the same and the best performance. In order to save memory and run time, only the 6 best features will be included in the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The six selected best features: \n",
      "                             Score\n",
      "exercised_stock_options  24.815080\n",
      "total_stock_value        24.182899\n",
      "bonus                    20.792252\n",
      "salary                   18.289684\n",
      "fraction_to_poi          16.409713\n",
      "deferred_income          11.458477\n",
      "\n",
      "The preformance before and after feature selection:\n",
      "                          Accuracy        F1  Precision  Recall\n",
      "Before feature selection  0.883721  0.444444        0.5     0.4\n",
      "After feature selection   0.880952  0.545455        0.6     0.5\n",
      "Improved:                -0.002769  0.101010        0.1     0.1\n"
     ]
    }
   ],
   "source": [
    "# select_k_best: number of features- 6\n",
    "new_features_list, score_list = select_k_best(data_dict, features_list, 6)\n",
    "print \"The six selected best features: \"\n",
    "select_k_best_table = pd.DataFrame({\"Score\": score_list}, index = new_features_list)\n",
    "print select_k_best_table\n",
    "print \"\"\n",
    "\n",
    "# update the feature list \n",
    "features_list = [\"poi\"] + new_features_list\n",
    "\n",
    "# The performance before and after the feature selection\n",
    "clf_afse = GaussianNB()\n",
    "clf_afse, accuracy, precision, recall, f1, f2 = performance(clf_afse, my_dataset, features_list)\n",
    "\n",
    "features_status_list = [\"Before feature selection\", \"After feature selection\", \"Improved:\"]\n",
    "accuracy_list = [accuracy_s, accuracy, -(accuracy_s - accuracy)]\n",
    "precision_list = [precision_s, precision, -(precision_s - precision)]\n",
    "recall_list =[recall_s, recall, -(recall_s - recall)]\n",
    "f1_list =[f1_s, f1, -(f1_s - f1)]\n",
    "     \n",
    "status_table = pd.DataFrame({\"Accuracy\": accuracy_list, \"Precision\": precision_list, \n",
    "                             \"Recall\": recall_list, \"F1\": f1_list}, index = features_status_list)\n",
    "print \"The preformance before and after feature selection:\"\n",
    "print status_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 6 selected best features are the \"Exercised Stock Options\", \"Total Stock Value\", \"Bouns\", \"Salary\", \"Fraction to POI\" and \"Deferred Income\". The preformance table has clearly showed that the accuary, precision and F1 scores have improved after including only the 6 best features to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preformance before and after feature selection:\n",
      "                      Accuracy        F1  Precision    Recall\n",
      "Gaussian Naive Bayes  0.880952  0.545455   0.600000  0.500000\n",
      "KMeans Clustering     0.880952  0.444444   0.666667  0.333333\n",
      "Decision Trees        0.857143  0.400000   0.500000  0.333333\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Classifiers.\n",
    "# Naive Bayes:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_n = GaussianNB()\n",
    "clf_n, accuracy_n, precision_n, recall_n, f1_n, f2_n = performance(clf_n, my_dataset, features_list)\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn import tree\n",
    "clf_d = tree.DecisionTreeClassifier()\n",
    "clf_d, accuracy_d, precision_d, recall_d, f1_d, f2_d = performance(clf_d, my_dataset, features_list)\n",
    "\n",
    "# K-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "clf_k = KMeans(n_clusters = 2)\n",
    "clf_k, accuracy_k, precision_k, recall_k, f1_k, f2_k = performance(clf_k, my_dataset, features_list)\n",
    "\n",
    "# The performance for algorithm selection\n",
    "classifiers_list = [\"Gaussian Naive Bayes\", \"KMeans Clustering\", \"Decision Trees\", ]\n",
    "accuracy_list = [accuracy_n, accuracy_k, accuracy_d] \n",
    "precision_list = [precision_n, precision_k, precision_d]\n",
    "recall_list =[recall_n, recall_k, recall_d]\n",
    "f1_list =[f1_n, f1_k, f1_d]\n",
    "     \n",
    "classifiers_table = pd.DataFrame({\"Accuracy\": accuracy_list, \"Precision\": precision_list, \n",
    "                             \"Recall\": recall_list, \"F1\": f1_list}, index = classifiers_list)\n",
    "print \"The preformance before and after feature selection:\"\n",
    "print classifiers_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different algorithms, Gaussian Naive Bayes, Decision Trees and KMeans Clustering, have been attempted. Gaussian Naive Bayes has the highest F1 score, thus has the best performance. Although Decision Trees's performance is pretty close to KMeans Clustering's, its performance is the worst among these three algorithms. \n",
    "\n",
    "Although Gaussian Naive Bayes has the best performance, it already has a decent preformance and only has one parameter to tune. As Decision Tree has the worst performance and it has several parameters, such as criterion, splitter, max_features, it will be tuned to improve its performance and compare the performance with the other two algorithms again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'criterion':('gini', 'entropy'),\n",
    "              'splitter':('best', 'random'), \n",
    "              'max_features':('auto', 'sqrt', 'log2')}\n",
    "clf_g = tree.DecisionTreeClassifier()\n",
    "clf_g = GridSearchCV(clf_g, parameters)\n",
    "clf_g, accuracy_a, precision_a, recall_a, f1_a, f2_a = performance(clf_g, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'splitter': 'best', 'criterion': 'gini'}\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "print clf_g.best_params_ \n",
    "print clf_g.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preformance before and after tuning:\n",
      "                            Accuracy        F1  Precision    Recall\n",
      "Gaussian Naive Bayes        0.880952  0.545455   0.600000  0.500000\n",
      "KMeans Clustering           0.880952  0.444444   0.666667  0.333333\n",
      "Decision Trees              0.857143  0.400000   0.500000  0.333333\n",
      "Decision Tree after tuning  0.857143  0.500000   0.500000  0.500000\n"
     ]
    }
   ],
   "source": [
    "# Performance of Decision Tree after tuning\n",
    "clf_g = tree.DecisionTreeClassifier(criterion='gini', max_features='auto', splitter='best')\n",
    "clf_g, accuracy_g, precision_g, recall_g, f1_g, f2_g = performance(clf_g, my_dataset, features_list)\n",
    "\n",
    "# The performance for algorithm selection\n",
    "classifiers_list = [\"Gaussian Naive Bayes\", \"KMeans Clustering\", \"Decision Trees\", \"Decision Tree after tuning\"]\n",
    "accuracy_list = [accuracy_n, accuracy_k, accuracy_d, accuracy_g] \n",
    "precision_list = [precision_n, precision_k, precision_d, precision_g]\n",
    "recall_list =[recall_n, recall_k, recall_d, recall_g]\n",
    "f1_list =[f1_n, f1_k, f1_d, recall_g]\n",
    "     \n",
    "classifiers_table = pd.DataFrame({\"Accuracy\": accuracy_list, \"Precision\": precision_list, \n",
    "                             \"Recall\": recall_list, \"F1\": f1_list}, index = classifiers_list)\n",
    "print \"The preformance before and after tuning:\"\n",
    "print classifiers_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms are parameterized and modification of those parameters can influence the outcome of the learning process. Tuning an algorithm is a process of finding the best parameters combination, and thus enable the algorithm to perform the best.\n",
    "\n",
    "GridSearchCV is attempted to tune the Decision Tree classifier. It is a way of systematically working through multiple combinations of parameter tunes and cross-validating to determine which tune gives the best performance. Three  Decision Tree parameters, criterion, splitter, max_features, have been tuned. And the performance of the Decision Tree classifier has improved. However, the performance of Gaussian Naive Bayes is still the best among these algorithms. Thus, it will be picked for final analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def validating_performance(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        return clf, accuracy, precision, recall, f1, f2\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of GaussianNB(priors=None)\n",
      "     Accuracy: 0.8605 ,Precision: 0.515719063545 ,Recall: 0.3855 ,F1: 0.441201716738\n"
     ]
    }
   ],
   "source": [
    "# Performance after validation\n",
    "clf_nv = GaussianNB()\n",
    "clf, accuracy, precision, recall, f1, f2 = validating_performance(clf_nv, my_dataset, features_list, folds = 1000)\n",
    "print_performance(clf, accuracy, precision, recall, f1, f2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is a process used to evaluate the trained model with a testing dataset. The goal of it is to test the generalization ability of a trained model. A dataset will be split into 2 subsets, the training and testing subset. The data in the training subset will be used to train the classifier, and new data from the testing subset will be used to estimate how well the classifier has been trained. If the same data has been used to both train and test a classifer, the classifier will be able to report the correct result almost every time when you ask it to classfy an instance. Because the model is making prediction to data that it has \"seen\" before an used to create the model. However, when lacking new data, data that is \"unseen\" by the model, to test the classfier, there are no indicators to show the accuracy of the algorithm, how well the model can generalize to new data, and whether or not the model overfits or underfits the training data. \n",
    "\n",
    "Stratified ShuffleSplit cross-validation has been used to perform validation. This validation method is a merge of Stratified K-Folds cross-validation and ShuffleSplit cross validation. This validator will return stratified randomized folds, and the folds will be made by preserving the percentage of samples for each class. Cross validation is adeqiate here as it can be to adjust and prevent the class imbalance problem in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
